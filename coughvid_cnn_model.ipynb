{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mFQdRmImpU7C"
      },
      "source": [
        "TRAINING CUSTOM CNN and PRETRAINED VGG CNN MODEL ON COUGHVID DATASET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln_M0Y2YDO7S"
      },
      "outputs": [],
      "source": [
        "# This notebook is based on this playlist -> https://www.youtube.com/playlist?list=PL-wATfeyAMNoirN4idjev6aRu8ISZYVWm\n",
        "# Please follow the playlist to understand the notebook in depth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je_XLNiLDO9u",
        "outputId": "4d5b8613-b4c8-4c2b-ce43-48b7dfd1b644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Jun  9 08:32:59 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EHaM3WDNpVmp"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/coughvid_cnn/balanced_dataset.zip > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BVD9korRpVpC"
      },
      "outputs": [],
      "source": [
        "cough_audio_data_path = \"/content/balanced_dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hTKB5gO6vGnX"
      },
      "outputs": [],
      "source": [
        "features_path = \"/content/drive/MyDrive/coughvid_cnn/metadata_cnn.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "gS0kwJGcvKmy",
        "outputId": "b886889c-9e2b-439a-abf8-f1431a48cf6e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2a2260bb-dee7-48bd-af6d-65be22f7d052\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>status</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>001328dc-ea5d-4847-9ccf-c5aa2a3f2d0f</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>001d8e33-a4af-4edb-98ba-b03f891d9a6c</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0029d048-898a-4c70-89c7-0815cdcf7391</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>002d28bc-7806-4dfb-9c9b-afa8cb623cac</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00357712-dd5a-4c0a-90a4-39f1f4b9d5fd</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a2260bb-dee7-48bd-af6d-65be22f7d052')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a2260bb-dee7-48bd-af6d-65be22f7d052 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a2260bb-dee7-48bd-af6d-65be22f7d052');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                   uuid  status\n",
              "0  001328dc-ea5d-4847-9ccf-c5aa2a3f2d0f       1\n",
              "1  001d8e33-a4af-4edb-98ba-b03f891d9a6c       0\n",
              "2  0029d048-898a-4c70-89c7-0815cdcf7391       2\n",
              "3  002d28bc-7806-4dfb-9c9b-afa8cb623cac       1\n",
              "4  00357712-dd5a-4c0a-90a4-39f1f4b9d5fd       0"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_parquet(features_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Kfv8MrAvRbn",
        "outputId": "48cffa0d-315a-4878-e23c-3016e5ba42e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 6469 entries, 0 to 6468\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   uuid    6469 non-null   object\n",
            " 1   status  6469 non-null   int32 \n",
            "dtypes: int32(1), object(1)\n",
            "memory usage: 75.9+ KB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-cHwQXHpVrk",
        "outputId": "c33197a3-f3a6-4371-a837-51a85c734662"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6469"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "status_labels_list = list(df.status)\n",
        "len(status_labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nVumGioTxtvS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_jv9NwXtuxa",
        "outputId": "ea357d5e-1a08-48b0-d50a-8384bb08eace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0oruyzoInZHb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fH0Cqt-ypVwE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pZhajbhbpVym"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r_qUcHEWpV09"
      },
      "outputs": [],
      "source": [
        "# Define the dataset class\n",
        "class CoughDataset(Dataset):\n",
        "    def __init__(self, audio_path, labels_path, transformation, target_sample_rate, num_samples):\n",
        "        self.audio_path = audio_path\n",
        "        self.labels_df = pd.read_parquet(labels_path)\n",
        "        #self.device = device\n",
        "        self.transformation = transformation  #.to(self.device)\n",
        "        self.target_sample_rate = target_sample_rate\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        audio_file_path = self._get_audio_path(index)\n",
        "        audio_label = self._get_audio_label(index)\n",
        "\n",
        "        signal, sample_rate = torchaudio.load(audio_file_path)\n",
        "        #signal = signal.to(self.device)\n",
        "        \n",
        "        signal = self._resample_if_necessary(signal, sample_rate)\n",
        "        signal = self._mix_down_if_necessary(signal)\n",
        "        signal = self._cut_if_necessary(signal)\n",
        "        signal = self._right_pad_if_necessary(signal)\n",
        "        signal = self.transformation(signal)\n",
        "        \n",
        "        return signal, audio_label\n",
        "\n",
        "    def _get_audio_path(self, index):\n",
        "        filename = f\"{self.labels_df.iloc[index,0]}\" +\".wav\"\n",
        "        path = os.path.join(self.audio_path,filename)\n",
        "        return path\n",
        "    \n",
        "    def _get_audio_label(self, index):\n",
        "        label = self.labels_df.iloc[index,1]\n",
        "        return label\n",
        "\n",
        "    def _resample_if_necessary(self, signal, sr):\n",
        "        if sr != self.target_sample_rate:\n",
        "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
        "            signal = resampler(signal)\n",
        "        return signal\n",
        "\n",
        "    def _mix_down_if_necessary(self, signal):\n",
        "        if signal.shape[0] > 1:\n",
        "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "        return signal\n",
        "\n",
        "    def _cut_if_necessary(self, signal):\n",
        "        if signal.shape[1] > self.num_samples:\n",
        "            signal = signal[:, :self.num_samples]\n",
        "        return signal\n",
        "\n",
        "    def _right_pad_if_necessary(self, signal):\n",
        "        length_signal = signal.shape[1]\n",
        "        if length_signal < self.num_samples:\n",
        "            num_missing_samples = self.num_samples - length_signal\n",
        "            last_dim_padding = (0, num_missing_samples)\n",
        "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
        "        return signal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IM1EfyncmEtM"
      },
      "outputs": [],
      "source": [
        "AUDIO_DIRECTORY = cough_audio_data_path\n",
        "AUDIO_LABELS_DIRECTORY = features_path\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "\n",
        "#if torch.cuda.is_available():\n",
        "#    device = \"cuda\"\n",
        "#else:\n",
        "#    device = \"cpu\"\n",
        "#print(f\"Using device {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNh72Z14JGNM",
        "outputId": "88095823-dad7-477a-a155-78c2a81c9a52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of files are equal to : 6469\n",
            "Healthy_label corresponds to label number: 1\n",
            "Covid_label corresponds to label number: 0\n",
            "Symptomatic_label corresponds to label number: 2\n"
          ]
        }
      ],
      "source": [
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=1024,\n",
        "        hop_length=512,\n",
        "        n_mels=64\n",
        "    ) \n",
        "\n",
        "# Create an instance of the dataset\n",
        "coughdata = CoughDataset(audio_path = AUDIO_DIRECTORY, labels_path = AUDIO_LABELS_DIRECTORY, transformation = mel_spectrogram, target_sample_rate = SAMPLE_RATE, num_samples = NUM_SAMPLES)\n",
        "\n",
        "print(f\"The number of files are equal to : {len(coughdata)}\")\n",
        "    \n",
        "healthy_audio, healthy_label = coughdata[0]\n",
        "print(f\"Healthy_label corresponds to label number: {healthy_label}\")\n",
        "\n",
        "cough_audio, covid_label = coughdata[1]\n",
        "print(f\"Covid_label corresponds to label number: {covid_label}\")\n",
        "\n",
        "symptomatic_audio, symptomatic_label = coughdata[2]\n",
        "print(f\"Symptomatic_label corresponds to label number: {symptomatic_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EXVLfSsb79U3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "B3geRHFN79XS"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "class CNNNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 4 conv blocks / flatten / linear / softmax\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=1,\n",
        "                out_channels=16,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=16,\n",
        "                out_channels=32,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=32,\n",
        "                out_channels=64,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=64,\n",
        "                out_channels=128,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding=2\n",
        "            ),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear = nn.Linear(128 * 5 * 4, 3)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        x = self.conv1(input_data)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear(x)\n",
        "        predictions = self.softmax(logits)\n",
        "        return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWxnmVNTQnh_",
        "outputId": "50d4a9ca-7db4-470a-ae41-6c5696f22087"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQLPHypy79Z7",
        "outputId": "e906e4b5-4610-403a-a7b8-36607725522e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 66, 46]             160\n",
            "              ReLU-2           [-1, 16, 66, 46]               0\n",
            "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
            "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
            "              ReLU-5           [-1, 32, 35, 25]               0\n",
            "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
            "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
            "              ReLU-8           [-1, 64, 19, 14]               0\n",
            "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
            "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
            "             ReLU-11           [-1, 128, 11, 9]               0\n",
            "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
            "          Flatten-13                 [-1, 2560]               0\n",
            "           Linear-14                    [-1, 3]           7,683\n",
            "          Softmax-15                    [-1, 3]               0\n",
            "================================================================\n",
            "Total params: 104,835\n",
            "Trainable params: 104,835\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.83\n",
            "Params size (MB): 0.40\n",
            "Estimated Total Size (MB): 2.24\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "cnn = CNNNetwork().to(device)\n",
        "summary(cnn, (1, 64, 44))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vMA2sIro79ca"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L7DRHk9tFHkL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MeRc5FnK79fH"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def create_data_loader(train_data, batch_size):\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    return train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XHobKwKXD6e8"
      },
      "outputs": [],
      "source": [
        "def train_single_epoch(model, data_loader, loss_fn, optimiser, device):\n",
        "    for input, target in data_loader:\n",
        "        target = target.type(torch.LongTensor)\n",
        "        input, target = input.to(device), target.to(device)\n",
        "\n",
        "        # calculate loss\n",
        "        prediction = model(input)\n",
        "        loss = loss_fn(prediction, target)\n",
        "\n",
        "        # backpropagate error and update weights\n",
        "        optimiser.zero_grad()\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "    print(f\"loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KC1kfVqCD6jP"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, loss_fn, optimiser, device, epochs):\n",
        "    for i in range(epochs):\n",
        "        print(f\"Epoch {i+1}\")\n",
        "        train_single_epoch(model, data_loader, loss_fn, optimiser, device)\n",
        "        print(\"---------------------------\")\n",
        "    print(\"Finished training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnqFLJD_D6n7",
        "outputId": "d8b231ce-9e71-4c56-824e-205cc6360e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n"
          ]
        }
      ],
      "source": [
        "AUDIO_DIRECTORY = cough_audio_data_path\n",
        "AUDIO_LABELS_DIRECTORY = features_path\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 20\n",
        "LEARNING_RATE = 0.003\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "        sample_rate=SAMPLE_RATE,\n",
        "        n_fft=1024,\n",
        "        hop_length=512,\n",
        "        n_mels=64\n",
        "    ) \n",
        "\n",
        "# Create an instance of the dataset\n",
        "coughdata = CoughDataset(audio_path = AUDIO_DIRECTORY, labels_path = AUDIO_LABELS_DIRECTORY, \\\n",
        "                            transformation = mel_spectrogram, \\\n",
        "                            target_sample_rate = SAMPLE_RATE, num_samples = NUM_SAMPLES)\n",
        "\n",
        "train_dataloader = create_data_loader(coughdata, BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOrxfWK9E3lN",
        "outputId": "8d75498c-c3c9-48de-f605-d62e5ab4556f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNNNetwork(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv3): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv4): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear): Linear(in_features=2560, out_features=3, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# construct model and assign it to device\n",
        "cnn = CNNNetwork().to(device)\n",
        "print(cnn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "zMblNXoMD6sd"
      },
      "outputs": [],
      "source": [
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr = LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbAMuckpD6x6",
        "outputId": "2094b504-dac6-40df-a68e-1229a76db779"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "loss: 0.9580729603767395\n",
            "---------------------------\n",
            "Epoch 2\n",
            "loss: 0.9711621999740601\n",
            "---------------------------\n",
            "Epoch 3\n",
            "loss: 0.9437869787216187\n",
            "---------------------------\n",
            "Epoch 4\n",
            "loss: 0.9628776907920837\n",
            "---------------------------\n",
            "Epoch 5\n",
            "loss: 0.9462932348251343\n",
            "---------------------------\n",
            "Epoch 6\n",
            "loss: 0.9532338976860046\n",
            "---------------------------\n",
            "Epoch 7\n",
            "loss: 0.9531334042549133\n",
            "---------------------------\n",
            "Epoch 8\n",
            "loss: 0.9749793410301208\n",
            "---------------------------\n",
            "Epoch 9\n",
            "loss: 0.947825014591217\n",
            "---------------------------\n",
            "Epoch 10\n",
            "loss: 0.9477611184120178\n",
            "---------------------------\n",
            "Epoch 11\n",
            "loss: 0.9694269299507141\n",
            "---------------------------\n",
            "Epoch 12\n",
            "loss: 0.9683235287666321\n",
            "---------------------------\n",
            "Epoch 13\n",
            "loss: 0.9715609550476074\n",
            "---------------------------\n",
            "Epoch 14\n",
            "loss: 0.9582096934318542\n",
            "---------------------------\n",
            "Epoch 15\n",
            "loss: 0.9777321219444275\n",
            "---------------------------\n",
            "Epoch 16\n",
            "loss: 0.9836634993553162\n",
            "---------------------------\n",
            "Epoch 17\n",
            "loss: 0.9733699560165405\n",
            "---------------------------\n",
            "Epoch 18\n",
            "loss: 0.9569403529167175\n",
            "---------------------------\n",
            "Epoch 19\n",
            "loss: 0.9636013507843018\n",
            "---------------------------\n",
            "Epoch 20\n",
            "loss: 1.0642236471176147\n",
            "---------------------------\n",
            "Finished training\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4_paaSBE712",
        "outputId": "a479ef38-23d3-431e-918e-1acd2aebcb83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained custom cnn model saved at cnnnet.pth\n"
          ]
        }
      ],
      "source": [
        "# save model\n",
        "torch.save(cnn.state_dict(), \"cnnnet.pth\")\n",
        "print(\"Trained custom cnn model saved at cnnnet.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QeeiQhsHE8Ha"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kBUaLCA4FAmU"
      },
      "outputs": [],
      "source": [
        "class_mapping = {\n",
        "    0 : \"covid\",\n",
        "    1 : \"healthy\",\n",
        "    2 : \"symptomatic\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "2pAKvQyZFf33"
      },
      "outputs": [],
      "source": [
        "def predict(model, input, target, class_mapping):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(input)\n",
        "        # Tensor (1, 10) -> [ [0.1, 0.01, ..., 0.6] ]\n",
        "        predicted_index = predictions[0].argmax(0)\n",
        "        #predicted = class_mapping[predicted_index]\n",
        "        #expected = class_mapping[target]\n",
        "        predicted =  predicted_index\n",
        "        expected = target\n",
        "    return predicted, expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkEcircCFf9N",
        "outputId": "ad39ce0d-bf53-4ee4-803d-edf4212a5882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: '1', expected: '1'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "state_dict = torch.load(\"cnnnet.pth\")\n",
        "cnn.load_state_dict(state_dict)\n",
        "\n",
        "# load urban sound dataset dataset\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=1024,\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "\n",
        "coughdata = CoughDataset(audio_path = AUDIO_DIRECTORY, labels_path = AUDIO_LABELS_DIRECTORY, \\\n",
        "                            transformation = mel_spectrogram, \\\n",
        "                            target_sample_rate = SAMPLE_RATE, num_samples = NUM_SAMPLES)\n",
        "\n",
        "\n",
        "# get a sample from the urban sound dataset for inference\n",
        "input, target = coughdata[0][0], coughdata[0][1] # [batch size, num_channels, fr, time]\n",
        "input.unsqueeze_(0)\n",
        "\n",
        "input_array = np.array(input)\n",
        "target_array = np.array(target)\n",
        "target_tensor = torch.from_numpy(target_array)\n",
        "target = target_tensor.type(torch.LongTensor)\n",
        "input, target = input.to(device), target.to(device)\n",
        "\n",
        "# make an inference\n",
        "predicted, expected = predict(cnn, input, target, class_mapping)\n",
        "print(f\"Predicted: '{predicted}', expected: '{expected}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "C5s0RkbFF4H6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NKHPywBu6GLm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-e06alti6GOo"
      },
      "outputs": [],
      "source": [
        "# PRETRAINING VGG 16 / VGG 19 MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "5J6Uz7YL6GRq"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "def build_model(model_type,image_size,input_channels,output_channels,device):\n",
        "    if model_type in ['vgg-16', 'vgg-19']:\n",
        "        assert image_size == 224\n",
        "        model = torchvision.models.vgg16(pretrained=True) if model_type == 'vgg-16' else torchvision.models.vgg19(pretrained=True)\n",
        "    if input_channels != 3:\n",
        "        first_conv_layer = [nn.Conv2d(input_channels, 3, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)]\n",
        "        first_conv_layer.extend(list(model.features))  \n",
        "        model.features= nn.Sequential(*first_conv_layer)\n",
        "\n",
        "\n",
        "    model.classifier[-1] = nn.Linear(4096, 1000)\n",
        "    model.classifier.add_module('7', nn.ReLU())\n",
        "    model.classifier.add_module('8', nn.Dropout(p=0.5, inplace=False))\n",
        "    model.classifier.add_module('9', nn.Linear(1000, output_channels))\n",
        "    model.classifier.add_module('10', nn.LogSoftmax(dim=1))\n",
        "            \n",
        "    for param in model.features[1:].parameters(): # disable grad for trained layers\n",
        "                param.requires_grad = False\n",
        "\n",
        "    return model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "gsLCSLdu6GUn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "fVJ6tuPNbaaB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTTuW4ctF4Ln",
        "outputId": "623e5437-e881-487f-e50e-3cd228cf44e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n",
            "100%|██████████| 548M/548M [00:03<00:00, 152MB/s]\n"
          ]
        }
      ],
      "source": [
        "model_type = \"vgg-19\"\n",
        "image_size = 224\n",
        "input_channels = 1\n",
        "output_channels = 3\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "model = build_model(model_type, image_size, input_channels, output_channels, device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "psngq69wb3-8"
      },
      "outputs": [],
      "source": [
        "vgg19 = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "I5Yv_7LkGXFb"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xx_qm9wgGXIS",
        "outputId": "65f897c6-26fb-4ce4-821b-30c01fdcf5e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "loss: 6.669395446777344\n",
            "---------------------------\n",
            "Epoch 2\n",
            "loss: 2.5267069339752197\n",
            "---------------------------\n",
            "Epoch 3\n",
            "loss: 1.2306019067764282\n",
            "---------------------------\n",
            "Epoch 4\n",
            "loss: 1.0254782438278198\n",
            "---------------------------\n",
            "Epoch 5\n",
            "loss: 1.005753993988037\n",
            "---------------------------\n",
            "Epoch 6\n",
            "loss: 0.9868350028991699\n",
            "---------------------------\n",
            "Epoch 7\n",
            "loss: 0.9913977980613708\n",
            "---------------------------\n",
            "Epoch 8\n",
            "loss: 1.0149961709976196\n",
            "---------------------------\n",
            "Epoch 9\n",
            "loss: 1.0409544706344604\n",
            "---------------------------\n",
            "Epoch 10\n",
            "loss: 1.0109081268310547\n",
            "---------------------------\n",
            "Epoch 11\n",
            "loss: 1.0119059085845947\n",
            "---------------------------\n",
            "Epoch 12\n",
            "loss: 1.0084298849105835\n",
            "---------------------------\n",
            "Epoch 13\n",
            "loss: 1.0118014812469482\n",
            "---------------------------\n",
            "Epoch 14\n",
            "loss: 1.0147223472595215\n",
            "---------------------------\n",
            "Epoch 15\n",
            "loss: 1.0116959810256958\n",
            "---------------------------\n",
            "Epoch 16\n",
            "loss: 1.0117095708847046\n",
            "---------------------------\n",
            "Epoch 17\n",
            "loss: 1.0158522129058838\n",
            "---------------------------\n",
            "Epoch 18\n",
            "loss: 1.0119355916976929\n",
            "---------------------------\n",
            "Epoch 19\n",
            "loss: 1.0116589069366455\n",
            "---------------------------\n",
            "Epoch 20\n",
            "loss: 1.0116428136825562\n",
            "---------------------------\n",
            "Epoch 21\n",
            "loss: 1.0116569995880127\n",
            "---------------------------\n",
            "Epoch 22\n",
            "loss: 1.0116273164749146\n",
            "---------------------------\n",
            "Epoch 23\n",
            "loss: 1.0116465091705322\n",
            "---------------------------\n",
            "Epoch 24\n",
            "loss: 1.0116900205612183\n",
            "---------------------------\n",
            "Epoch 25\n",
            "loss: 1.011722207069397\n",
            "---------------------------\n",
            "Epoch 26\n",
            "loss: 1.0135232210159302\n",
            "---------------------------\n",
            "Epoch 27\n",
            "loss: 1.0117669105529785\n",
            "---------------------------\n",
            "Epoch 28\n",
            "loss: 1.0102983713150024\n",
            "---------------------------\n",
            "Epoch 29\n",
            "loss: 1.010250449180603\n",
            "---------------------------\n",
            "Epoch 30\n",
            "loss: 1.011615514755249\n",
            "---------------------------\n",
            "Finished training\n"
          ]
        }
      ],
      "source": [
        "# train model\n",
        "train(vgg19, train_dataloader, criterion, optimizer, device, EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALjiXlGaGXLL",
        "outputId": "a92c95dc-c3cc-4280-dc38-a0ac2d3d85e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trained vgg19 model saved at vgg19net.pth\n"
          ]
        }
      ],
      "source": [
        "# save model\n",
        "torch.save(cnn.state_dict(), \"vgg19net.pth\")\n",
        "print(\"Trained vgg19 model saved at vgg19net.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "fl9pQUetUSuq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrvnPXWSUSxP",
        "outputId": "15f9c3dc-6f53-4d30-a34f-8368ba4c5c57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: '1', expected: '1'\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "state_dict = torch.load(\"vgg19net.pth\")\n",
        "cnn.load_state_dict(state_dict)\n",
        "\n",
        "# load urban sound dataset dataset\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=1024,\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "\n",
        "coughdata = CoughDataset(audio_path = AUDIO_DIRECTORY, labels_path = AUDIO_LABELS_DIRECTORY, \\\n",
        "                            transformation = mel_spectrogram, \\\n",
        "                            target_sample_rate = SAMPLE_RATE, num_samples = NUM_SAMPLES)\n",
        "\n",
        "\n",
        "# get a sample from the urban sound dataset for inference\n",
        "input, target = coughdata[0][0], coughdata[0][1] # [batch size, num_channels, fr, time]\n",
        "input.unsqueeze_(0)\n",
        "\n",
        "input_array = np.array(input)\n",
        "target_array = np.array(target)\n",
        "target_tensor = torch.from_numpy(target_array)\n",
        "target = target_tensor.type(torch.LongTensor)\n",
        "input, target = input.to(device), target.to(device)\n",
        "\n",
        "# make an inference\n",
        "predicted, expected = predict(cnn, input, target, class_mapping)\n",
        "print(f\"Predicted: '{predicted}', expected: '{expected}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "KWp5g_HkUSzq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rkOU2T76US2I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "hkDXEgnnpWuk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
